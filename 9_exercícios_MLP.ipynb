{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmoura/ED/blob/master/9_exerc%C3%ADcios_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 1 - Problema do XOR\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# 1- Importar o modelo MLP do sklearn\n",
        "# 2- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até 100% de acerto\n",
        "# 3- Treinar o modelo com os dados de treinamento\n",
        "# 4- Fazer o predict com os dados de treinamento\n",
        "# 5- Comparar o resultado do predict com o vetor y\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(4,3), activation='relu',max_iter=5000)\n",
        "model = model.fit(x, y)\n",
        "\n",
        "display(model.predict(x))"
      ],
      "metadata": {
        "id": "Q8khw7gpctj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 2 - Dataset Penguins\n",
        "\n",
        "# 1- Carregar a base de dados Penguins da API do Seaborn\n",
        "# 2- A base precisará ser tratada: Existem valores nulos e dados categóricos\n",
        "# 3- Normalizar todas as colunas (normalizar é deixar todos os valores das colunas entre 0 e 1) --> Redes Neurais são sensíveis as diferenças escalares de valores das features\n",
        "# 4- Separar o dataset em X (matriz de features) e y (coluna target)\n",
        "# 5- Gerar as bases de treinamento e teste\n",
        "# 6- Importar o modelo MLP do sklearn\n",
        "# 7- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até que obtenha uma taxa de acerto estável\n",
        "# 8- Treinar o modelo com os dados de treinamento\n",
        "# 9- Fazer o predict com os dados de teste\n",
        "# 10- Imprimir o percentual de acerto da base de teste\n",
        "from seaborn import load_dataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar base de dados\n",
        "df = load_dataset('penguins')\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "column_transformer = make_column_transformer((OneHotEncoder(), ['island','sex']), remainder='passthrough')\n",
        "df = column_transformer.fit_transform(df)\n",
        "df = pd.DataFrame(data=df)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[5]= label_encoder.fit_transform(df[5])\n",
        "\n",
        "df[6] = MinMaxScaler().fit_transform(np.array(df[6]).reshape(-1,1))\n",
        "df[7] = MinMaxScaler().fit_transform(np.array(df[7]).reshape(-1,1))\n",
        "df[8] = MinMaxScaler().fit_transform(np.array(df[8]).reshape(-1,1))\n",
        "df[9] = MinMaxScaler().fit_transform(np.array(df[9]).reshape(-1,1))\n",
        "\n",
        "y = df[5]\n",
        "X = df.drop(5,axis=1)\n",
        "\n",
        "display(X)"
      ],
      "metadata": {
        "id": "L0v15WSn9gS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1ILett_UCmM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None, stratify=y) # 80% treino e 20% teste\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(10,5), activation='tanh',max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "result = model.predict(X_test)\n",
        "acc = metrics.accuracy_score(result, y_test)\n",
        "show = round(acc * 100)\n",
        "print(\"{}%\".format(show))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 3 - Dataset Phoneme\n",
        "\n",
        "# 1- Carregar a base \"phoneme\"\n",
        "url = \"https://raw.githubusercontent.com/tmoura/machinelearning/master/phoneme.data\"\n",
        "\n",
        "# 2- A coluna 0 é o target\n",
        "# 3- Todas as colunas são numéricas e não possui valores nulos\n",
        "# 4- Separar o dataset em X (matriz de features) e y (coluna target)\n",
        "# 5- Gerar as bases de treinamento e teste\n",
        "# 6- Importar o modelo MLP do sklearn\n",
        "# 7- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até que obtenha uma taxa de acerto estável\n",
        "# 8- Treinar o modelo com os dados de treinamento\n",
        "# 9- Fazer o predict com os dados de teste\n",
        "# 10- Imprimir o percentual de acerto da base de teste"
      ],
      "metadata": {
        "id": "-w3Yk186ViXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}